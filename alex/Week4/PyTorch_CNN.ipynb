{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import os \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "num_epochs = 5\n",
    "num_classes = 10\n",
    "batch_size = 100\n",
    "learning_rate = 0.001\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "\n",
    "DATA_PATH = current_dir\n",
    "MODEL_STORE_PATH = current_dir\n",
    "trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root=DATA_PATH, train=False, transform=trans)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ConvNet, self).__init__()\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2))\n",
    "        self.drop_out = nn.Dropout()\n",
    "        self.fc1 = nn.Linear(7 * 7 * 64, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.reshape(out.size(0), -1)\n",
    "        out = self.drop_out(out)\n",
    "        out = self.fc1(out)\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ConvNet()\n",
    "\n",
    "# Loss and Adam optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.1336, Accuracy: 96.00%\n",
      "Epoch [1/5], Step [200/600], Loss: 0.1172, Accuracy: 96.00%\n",
      "Epoch [1/5], Step [300/600], Loss: 0.0159, Accuracy: 100.00%\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0873, Accuracy: 96.00%\n",
      "Epoch [1/5], Step [500/600], Loss: 0.0827, Accuracy: 95.00%\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1011, Accuracy: 97.00%\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0203, Accuracy: 99.00%\n",
      "Epoch [2/5], Step [200/600], Loss: 0.0491, Accuracy: 98.00%\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0400, Accuracy: 99.00%\n",
      "Epoch [2/5], Step [400/600], Loss: 0.0461, Accuracy: 98.00%\n",
      "Epoch [2/5], Step [500/600], Loss: 0.0133, Accuracy: 99.00%\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0214, Accuracy: 99.00%\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0764, Accuracy: 98.00%\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0276, Accuracy: 99.00%\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0129, Accuracy: 99.00%\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0874, Accuracy: 96.00%\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0222, Accuracy: 99.00%\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0244, Accuracy: 100.00%\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0119, Accuracy: 100.00%\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0936, Accuracy: 96.00%\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0137, Accuracy: 100.00%\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0182, Accuracy: 99.00%\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0423, Accuracy: 99.00%\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0062, Accuracy: 100.00%\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0334, Accuracy: 98.00%\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0434, Accuracy: 99.00%\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0234, Accuracy: 99.00%\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0099, Accuracy: 100.00%\n",
      "Epoch [5/5], Step [500/600], Loss: 0.0018, Accuracy: 100.00%\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0030, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "acc_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Run the forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "        # Backprop and perform Adam optimisation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track the accuracy\n",
    "        total = labels.size(0)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        correct = (predicted == labels).sum().item()\n",
    "        acc_list.append(correct / total)\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n",
    "                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n",
    "                          (correct / total) * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 98.97 %\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format((correct / total) * 100))\n",
    "\n",
    "# Save the model and plot\n",
    "torch.save(model.state_dict(), MODEL_STORE_PATH + '/conv_net_model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataiter = iter(test_loader)\n",
    "images, labels = dataiter.next()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "imgidx = 6\n",
    "img = images[imgidx][0]\n",
    "plt.imshow(img)\n",
    "output = model(images)\n",
    "print(torch.max(output.data,1)[1][imgidx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "        -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "        -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242, -0.4242,\n",
       "        -0.4242, -0.4242, -0.4242, -0.4242])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images[0][0][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
