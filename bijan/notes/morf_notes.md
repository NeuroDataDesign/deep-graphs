### intro

Decision forests are a big deal in classificaiton and regression.
Since forests are ensemble methods, each tree only needs to perform slightly better than chance for a lot of limit theorems to kick in.
Decision trees are pretty interpretabble since you can see which features they chose to split the data, and how they did it. BUT, the limited flexibility leads to overfitting.

Sparse Projection Oblique Randomer Forest (SPORF) is very good. RFs and other ML algs neglect order in the data where structure might contain some information. Previous methods extending RFs to vision tasks either generate features a priori from pixels, or lack flexibility to learn relevant patches. Manifold forests exists but only in low D, and unsupervised.

They propose a SPORF where at each node, they select a set of random spatially contiguous features using knowledge of the underlying manifold, and sum the intensities.

(note, skipping background since it's basically the same as in SPORF)

### Methods

in the structured setting, the projection vectors are selected to take advantage of the manifold which the data lie on. In the case of two-dimensional arrays, an observation is a vectorized representation of a data matrix. MF creates projections by summing intensities of rectangular patches of pixels.

Patches are generated by sampling an upper left corder, then the hight and width are directly sampled from uniform distributions. In this case, they call each patch an "atom". MF samples $d$ atoms, so $d$ features per observation, and selects the best one to split on.

### Simulations
#### 1D
Each observation is a discretization of a circle into 100 features with 2 non-adjacent segments of 1's. Class 1 is two lengths of 5 while class 2 is one length 4 and one length 6. Patches MF took were between 1 and 15 long, MF considers 40 at each node.

#### 2D
A simple 28x28 binary image with either random horizontal bars, or random vertical bars. $k \sim Poisson(10)$ bars were placed on each image. Projeciton patches were between 1 and 4 pixels in width and height, MF considers 28 at each node.

#### signal
one class is 100 values of gaussian noise, while the other class has an added exponentially decaying unit step starting at time 20. Patches were 1d with a width between 1 and 5 timesteps, each node coniders default number of splits (200).

#### Results
We can see that MF outperforms other algorithms in all settings except the images, where a CNN does better. Time takes is around the same as all other algs.

### Discussion
Note that MF also has feature importances which are basically smoothed versions of RF feature importances. Unlike other manifold forest methods, MF doesn't try to learn the manifold, but uses a priori knowledge of the manifold to improve classification accuracy.
