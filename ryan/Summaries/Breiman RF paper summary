- ensemble of trees have worked well for classification accuracy
- a random forest is a classifer consisting of a collection of trees where each tree casts a vote for the most popular class
- random forests do not overfit as more trees are added
- the generalization error of RF is dependent on the strength of individual classifiers in the forest and their correleation
- consequently, randomness added to an ensemble system must minimize correlation and maximize strength to improve accuracy


- Breiman's method: bagging (using random subset of the training set for an individual tree) and random feature selection are used
- bagging helps enhance accuracy as well as estimate generalization error (out-of-bag estimate)
- out-of-bad estimate: aggregate votes in which classifier is incorrect -> error rate
- a single random variable random forest is enough to have comparable results to Adaboost; having more random input variable does not increase accuracy substantially

- Forest-RC: instead of just using random features, take a random linear combinations of some features
- on smaller data sets, you do not need that many features to split on